from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import requests
from pymongo import MongoClient
from bson import ObjectId
import chromadb
from sentence_transformers import SentenceTransformer
import time
import re

app = FastAPI(
    title="Chatbot API",
    description="API tÃ­ch há»£p MongoDB + ChromaDB vÃ  llama.cpp",
    version="1.1.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

# ---------------------------
# 1. Káº¿t ná»‘i MongoDB
# ---------------------------
MONGO_URI = "mongodb://localhost:27017/"
DB_NAME = "Diary"

mongo_client = MongoClient(MONGO_URI)
db = mongo_client[DB_NAME]
chats_collection = db["chats"]

# ---------------------------
# 2. Káº¿t ná»‘i ChromaDB
# ---------------------------
chroma_client = chromadb.PersistentClient(path="./chromadb_store")
chat_vectors = chroma_client.get_or_create_collection(name="chat_embeddings")

# ---------------------------
# 3. Táº£i model embedding
# ---------------------------
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

def get_embedding(text: str):
    return embedding_model.encode(text).tolist()

# ---------------------------
# 4. Gá»i API cá»§a llama.cpp
# ---------------------------
LLAMA_API_URL = "http://127.0.0.1:8090/completion"

def call_llama(prompt: str) -> str:
    payload = {
        "prompt": prompt,
        "n_predict": 400,          # Sá»‘ token dá»± Ä‘oÃ¡n, cÃ³ thá»ƒ Ä‘iá»u chá»‰nh náº¿u cáº§n
        "temperature": 0.6,        # Giáº£m nhiá»‡t Ä‘á»™ Ä‘á»ƒ giáº£m tÃ­nh ngáº«u nhiÃªn, pháº£n há»“i á»•n Ä‘á»‹nh hÆ¡n
        "top_k": 40,               # Sá»‘ lá»±a chá»n tá»« phÃ­a trÃªn, giá»¯ nguyÃªn náº¿u cáº§n sá»± Ä‘a dáº¡ng vá»«a Ä‘á»§
        "top_p": 0.95,             # TÄƒng top_p Ä‘á»ƒ cho phÃ©p má»™t pháº¡m vi tá»« rá»™ng hÆ¡n, nhÆ°ng váº«n kiá»ƒm soÃ¡t Ä‘Æ°á»£c sá»± Ä‘a dáº¡ng
        "repeat_last_n": 128,      # Giáº£m sá»‘ token cuá»‘i Ä‘á»ƒ háº¡n cháº¿ láº·p láº¡i
        "repeat_penalty": 1.1,     # TÄƒng nháº¹ repeat_penalty Ä‘á»ƒ háº¡n cháº¿ láº·p láº¡i
        "stream": False,
    }
    try:
        resp = requests.post(LLAMA_API_URL, json=payload, timeout=30)
        return resp.json().get("content", "No response")
    except Exception as e:
        print(f"Error calling llama.cpp: {e}")
        return "Error calling AI"

# ---------------------------
# 5. Model cho API request
# ---------------------------
class ChatRequest(BaseModel):
    chatId: str  # ID cá»§a cuá»™c chat (MongoDB)
    message: str  # Tin nháº¯n cá»§a user

# ---------------------------
# 6. HÃ m xá»­ lÃ½ há»™i thoáº¡i AI
# ---------------------------
def save_message(chatId, user_message, ai_response):
    """LÆ°u tin nháº¯n cá»§a user vÃ  pháº£n há»“i cá»§a AI riÃªng biá»‡t vÃ o ChromaDB."""
    timestamp = int(time.time() * 1000)
    
    # LÆ°u tin nháº¯n cá»§a user
    user_text = f"User: {user_message}"
    user_embedding = get_embedding(user_text)
    chat_vectors.add(
        ids=[f"{chatId}-user-{timestamp}"],
        embeddings=[user_embedding],
        documents=[user_text],
        metadatas=[{"chatId": chatId, "role": "user", "timestamp": timestamp}]
    )
    
    # LÆ°u tin nháº¯n cá»§a AI
    ai_text = f"Bot: {ai_response}"
    ai_embedding = get_embedding(ai_text)
    chat_vectors.add(
        ids=[f"{chatId}-ai-{timestamp}"],
        embeddings=[ai_embedding],
        documents=[ai_text],
        metadatas=[{"chatId": chatId, "role": "ai", "timestamp": timestamp}]
    )

def clean_response(response):
    # Loáº¡i bá» dÃ²ng báº¯t Ä‘áº§u vá»›i "User:" hoáº·c "Bot:"
    cleaned = re.sub(r'^(User:|Bot:).*$', '', response, flags=re.MULTILINE)
    return cleaned.strip()

def retrieve_context(chatId, user_message, n_results=5):
    """
    Láº¥y n tin nháº¯n gáº§n nháº¥t cá»§a user tá»« ChromaDB Ä‘á»ƒ táº¡o bá»‘i cáº£nh há»™i thoáº¡i.
    Chá»‰ truy váº¥n cÃ¡c tin nháº¯n cÃ³ metadata role lÃ  'user'.
    Sau Ä‘Ã³, loáº¡i bá» cÃ¡c báº£n sao trÃ¹ng láº·p vÃ  chá»‰ láº¥y 5 tin nháº¯n gáº§n nháº¥t.
    """
    embedding = get_embedding(user_message)

    # Sá»­ dá»¥ng toÃ¡n tá»­ $and Ä‘á»ƒ káº¿t há»£p Ä‘iá»u kiá»‡n
    results = chat_vectors.query(
        query_embeddings=[embedding],
        n_results=n_results * 2,  # láº¥y nhiá»u hÆ¡n Ä‘á»ƒ Ä‘áº£m báº£o sau lá»c Ä‘á»§ 5 tin nháº¯n
        where={"$and": [{"chatId": chatId}, {"role": "user"}]}
    )
    
    related_texts = results.get("documents", [[]])[0] or []
    print("realated text: ")
    print(related_texts)
    # Loáº¡i bá» cÃ¡c tin nháº¯n trÃ¹ng láº·p, giá»¯ thá»© tá»± ban Ä‘áº§u
    unique_texts = list(dict.fromkeys(related_texts))
    print("unique text: ")
    print(unique_texts)
    # Chá»‰ láº¥y 5 tin nháº¯n gáº§n nháº¥t (cÃ³ thá»ƒ cáº§n sáº¯p xáº¿p láº¡i náº¿u cáº§n)
    context_lines = unique_texts[-n_results:]
    
    print("context lines: ")
    print(context_lines)
    return "\n".join(context_lines)

def create_prompt(chatId, user_message):
    """Táº¡o prompt hoÃ n chá»‰nh vá»›i ngá»¯ cáº£nh há»™i thoáº¡i."""
    context = retrieve_context(chatId, user_message)  # Láº¥y 5 tin nháº¯n gáº§n nháº¥t cá»§a user tá»« ChromaDB
    
    system_prompt = (
        "ğŸŒŸ You are an emotional support AI. Provide empathetic, constructive responses with lots of emojis. ğŸŒŸ\n"
        "Rules:\n"
        "1. ğŸ’– Show empathy before giving advice.\n"
        "2. ğŸ” For personal info queries, check history; if missing, reply: 'I donâ€™t know that yet. Could you tell me?'\n"
        "3. ğŸš« Do not mention your lack of physical form unless asked.\n"
        "4. ğŸ”„ Summarize past messages instead of repeating them.\n"
        "5. ğŸ‰ Use as many emojis as possible in your responses.\n"
    )


    if context.strip():
        final_prompt = f"{system_prompt}\nPrevious user messages:\n{context}\nUser just said: {user_message}\nYou:"
    else:
        final_prompt = f"{system_prompt}\nUser just said: {user_message}\nYou:"

    
    return final_prompt

# ---------------------------
# 7. Endpoint: API chÃ­nh cá»§a AI Chatbot
# ---------------------------
@app.post("/api/chats/ai")
def chat_api(req: ChatRequest):
    final_prompt = create_prompt(req.chatId, req.message)
    print("Prompt gá»­i Ä‘áº¿n llama.cpp:")
    print(final_prompt)
    
    ai_reply = call_llama(final_prompt)
    save_message(req.chatId, req.message, ai_reply)  # LÆ°u há»™i thoáº¡i vÃ o DB
    print("Pháº£n há»“i tá»« AI:")
    print(ai_reply)
    # nÃ y nÃ³ pháº£n há»“i táº­n 2 láº§, mÃ  trong khi t in print(..) cÃ³ 1 , m coi thá»­ coi coi cÃ¡i API cá»§a nháº­t kÃ­ Ã¡, thÆ°á»ng bá»‹ cÃ¡i nháº­t kÃ­
    # Xá»­ lÃ½ loáº¡i bá» dÃ²ng trá»‘ng thá»«a
    ai_reply = re.sub(r'\n\s*\n+', '\n', ai_reply.strip())
    # ai_reply = clean_response(ai_reply)
    
    return {"response": ai_reply}

# ---------------------------
# 8. Cháº¡y Server trÃªn port 4000
# ---------------------------
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=4000)
